{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c765e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/buka2004/PTQ-LLM-MIPT/.venv/bin/pip\n",
      "/home/buka2004/PTQ-LLM-MIPT/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which pip && which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300dded",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9766b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from re import L\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "# from transformers.file_utils import get_full_repo_name\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "import deepspeed\n",
    "from deepspeed.compression.compress import init_compression, redundancy_clean\n",
    "from deepspeed.compression.helper import recursive_getattr\n",
    "from deepspeed.compression.helper import convert_conv1d_to_linear\n",
    "\n",
    "import numpy as np\n",
    "from transformers.modeling_utils import Conv1D\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from smoothquant.smoothquant.calibration import get_act_scales\n",
    "from smoothquant.smoothquant.smooth import smooth_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997a75a",
   "metadata": {},
   "source": [
    "# Set python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PYTHONPATH=\"/home/buka2004/PTQ-LLM-MIPT:$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513ff92",
   "metadata": {},
   "source": [
    "# Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG='/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/config/ds_config_W8A8_Qgroup64_fp32.json'\n",
    "SAVE_PATH='/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_saving'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9f411",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38999d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 69a6e7d5-ca5d-4cc5-ae7d-0d91714b665c)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2-large/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "MODEL='openai-community/gpt2-large'\n",
    "config = AutoConfig.from_pretrained(CONFIG)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    from_tf=bool(\".ckpt\" in MODEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ab20088b-53ec-422c-b554-37f5b57b90b3)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2-large/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: /home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_saving\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL = 'openai-community/gpt2-large'\n",
    "CONFIG = '/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/config/ds_config_W8A8_Qgroup64_fp32.json'\n",
    "SAVE_PATH = '/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_saving'\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import deepspeed\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL,\n",
    "#     from_tf=bool(\".ckpt\" in MODEL),\n",
    "# )\n",
    "\n",
    "# Initialize DeepSpeed with quantization config and required training parameters\n",
    "ds_config = {\n",
    "    \"train_batch_size\": 1,  # Required by DeepSpeed\n",
    "    \"train_micro_batch_size_per_gpu\": 1,  # Required by DeepSpeed\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"steps_per_print\": 1,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 0,  # Disable ZeRO for quantization\n",
    "    },\n",
    "    \"quantization\": {\n",
    "        \"enabled\": True,\n",
    "        \"quantize_weight_in_forward\": False,\n",
    "        \"quantize_groups\": 64,\n",
    "        \"wq8\": True,\n",
    "        \"aq8\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.float16)\n",
    "ds_engine, _, _, _ = deepspeed.initialize(model=model, config_params=ds_config)\n",
    "lean_state_dict = deepspeed.checkpoint.utils.clone_tensors_for_torch_save(ds_engine.module.state_dict())\n",
    "ds_engine.module.save_pretrained(\"lean_after\", state_dict=lean_state_dict)\n",
    "\n",
    "# # Initialize DeepSpeed model\n",
    "# model_engine, _, _, _ = deepspeed.initialize(\n",
    "#     model=model,\n",
    "#     config=ds_config\n",
    "# )\n",
    "\n",
    "# model_engine.save_checkpoint(SAVE_PATH)\n",
    "\n",
    "print(f\"Quantized model saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b455e7de-cbb4-4e01-aeff-4ff104ac5ca1)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2-large/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Exception ignored in: <function DeepSpeedEngine.__del__ at 0x7f70a476f040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/buka2004/PTQ-LLM-MIPT/.venv/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 527, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Conv1D to Linear...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting Conv1D to Linear...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_conv1d_to_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying compression with modified config...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m init_compression(model, modified_config_path)\n",
      "File \u001b[0;32m~/PTQ-LLM-MIPT/.venv/lib/python3.8/site-packages/deepspeed/compression/helper.py:245\u001b[0m, in \u001b[0;36mconvert_conv1d_to_linear\u001b[0;34m(model, convert_type)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, convert_type):\n\u001b[1;32m    244\u001b[0m     old_module \u001b[38;5;241m=\u001b[39m recursive_getattr(c_model, name)\n\u001b[0;32m--> 245\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mold_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mold_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     new_module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m old_module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PTQ-LLM-MIPT/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PTQ-LLM-MIPT/.venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:110\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/PTQ-LLM-MIPT/.venv/lib/python3.8/site-packages/torch/nn/init.py:460\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    458\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Alternative: Save quantized model by creating a modified config\n",
    "that stores quantized weights in the state dict.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from transformers.modeling_utils import Conv1D\n",
    "from deepspeed.compression.compress import init_compression, redundancy_clean\n",
    "from deepspeed.compression.helper import convert_conv1d_to_linear\n",
    "\n",
    "# Configuration\n",
    "MODEL = 'openai-community/gpt2-large'\n",
    "CONFIG_PATH = '/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/config/ds_config_W8A8_Qgroup64_fp32.json'\n",
    "SAVE_PATH = '/home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_saving'\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Load and modify config to disable runtime quantization\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "# Set quantize_weight_in_forward to False to store quantized weights\n",
    "ds_config['compression_training']['weight_quantization']['shared_parameters']['quantize_weight_in_forward'] = False\n",
    "\n",
    "# Save modified config\n",
    "modified_config_path = os.path.join(SAVE_PATH, 'ds_config_modified.json')\n",
    "with open(modified_config_path, 'w') as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "print(\"Converting Conv1D to Linear...\")\n",
    "model = convert_conv1d_to_linear(model, Conv1D)\n",
    "\n",
    "print(\"Applying compression with modified config...\")\n",
    "model = init_compression(model, modified_config_path)\n",
    "\n",
    "print(\"Cleaning redundant wrappers...\")\n",
    "model = redundancy_clean(model, modified_config_path)\n",
    "\n",
    "print(\"\\nSaving quantized model...\")\n",
    "# Save the model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "state_dict = model_to_save.state_dict()\n",
    "\n",
    "# Save state dict\n",
    "torch.save(state_dict, os.path.join(SAVE_PATH, 'pytorch_model.bin'))\n",
    "\n",
    "# Save config and tokenizer\n",
    "model_to_save.config.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "# Report size\n",
    "checkpoint_size = os.path.getsize(os.path.join(SAVE_PATH, 'pytorch_model.bin'))\n",
    "checkpoint_size_mb = checkpoint_size / (1024 * 1024)\n",
    "checkpoint_size_gb = checkpoint_size / (1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model saved to: {SAVE_PATH}\")\n",
    "print(f\"Checkpoint size: {checkpoint_size:,} bytes ({checkpoint_size_mb:.2f} MB / {checkpoint_size_gb:.2f} GB)\")\n",
    "print(f\"\\nTo load:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{SAVE_PATH}')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
