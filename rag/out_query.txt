Using following context:

the problematic updates. 3.1 W ARMUP AS VARIANCE REDUCTION In this section, we design a set of controlled experiments to verify our hypothesis. Particularly, we design two variants of Adam that reducing the variance of the adaptive learning rate: Adam-2k and Adam-eps. We compare them to vanilla Adam with and without warmup on the IWSLT’14 German to English translation dataset (Cettolo et al., 2014). In order to reduce the variance of the adaptive learning rate (ψ(.)), Adam-2k only updatesψ(.) in the ﬁrst two thousand iterations, while the momentum ( φ(.)) and parameters ( θ) are ﬁxed 3; other than this, it follows the original Adam algorithm. To make comparison with other methods, its iterations are indexed from -1999 instead of 1. In Figure 1, we observe that, after getting these additional two thousand samples for estimating the adaptive learning rate, Adam-2k avoids the convergence problem of the vanilla-Adam. Also, comparing Figure 2 and Figure 3, getting large enough samples prevents the gradient distribution from being distorted. These observations verify our hypothesis that the lack of sufﬁcient data samples in the early stage is the root cause of the convergence issue. 2The mean zero normal assumption is valid at the beginning of the training, since weights are sampled from normal distributions with mean zero (Balduzzi et al., 2017), further analysis is conducted in Section 5.3. 3Different from Gotmare et al. (2019), all parameters and ﬁrst moments are frozen in the ﬁrst 2000 iterations. 3 Published as a conference paper at ICLR 2020 Another straightforward way to reduce

in Figure 3). However, as in Figure 1, it produces a much worse performance comparing to Adam-2k and Adam-warmup. We conjecture that this is because large ϵ induces a large bias into the adaptive learning rate and slows down the optimization process. Thus, we need a more principled and rigorous way to con- trol the variance of the adaptive learning rate. In the next subsection, we will present a theoretical analysis of the variance of the adaptive learning rate. 3.2 A NALYSIS OF ADAPTIVE LEARNING RATE VARIANCE As mentioned before, Adam uses the exponential moving average to calculate the adaptive learning rate. For gradients {g1,··· ,gt}, their exponential moving average has a larger variance than their simple average. Also, in the early stage ( t is small), the difference of the exponential weights of {g1,··· ,gt}is relatively small (up to 1 −βt−1 2 ). Therefore, for ease of analysis, we approximate the distribution of the exponential moving average as the distribution of the simple average (Nau, 2014), i.e., p(ψ(.)) = p( √ 1−βt 2 (1−β2) ∑t i=1 βt−i 2 g2 i ) ≈p( √ t∑t i=1 g2 i ). Since gi ∼N (0,σ2), we have t∑t i=1 g2 i ∼Scale-inv-X2(t, 1 σ2 ). Therefore, we assume 1−βt 2 (1−β2) ∑t i=1 βt−i 2 g2 i also subjects to a scaled inverse chi-square distribution with ρdegrees of freedom (further analysis on this approximation is conducted in Section 5.3). Based on this assumption, we can calculate Var[ψ2(.)] and the PDF of ψ2(.). Now, we proceed to the analysis

Figure 7: Performance of RAdam, Adam with warmup on CIFAR10 with different learning rates. 5.1 C OMPARING TO VANILLA ADAM As analyzed before, the adaptive learning rate has undesirably large variance in the early stage of training and leads to suspicious/bad local optima on NMT. One question we are interested in is: whether such an issue widely exits in other similar tasks and applications. Thus, we conduct a set of experiments with two classical tasks of NLP and CV , i.e., language modeling and image classiﬁcation. RAdam not only results in consistent improvements over the vanilla Adam, but also demonstrates its robustness to the change of learning rates. It veriﬁes that the variance issue exists in various machine learning applications, and has a big impact on the model behavior. Performance Comparison. The performances on language modeling ( i.e., One Billion Word (Chelba et al., 2013)) and image classiﬁcation ( i.e., CIFAR10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009)) are presented in Figure 4, 5. The results show that RAdam out- performs Adam in all three datasets. As shown in Figure 4, although the rectiﬁcation term makes RAdam slower than the vanilla Adam in the ﬁrst few epochs, it allows RAdam to converge faster after that. In other words, by reducing the variance of the adaptive learning rate in the early stage, it gets both faster convergence and better performance, which veriﬁes the impact of the variance issue. We also observe that RAdam obtains consistent improvements over Adam on image classiﬁcation. It is worth

Answer on query: 
how adam works?