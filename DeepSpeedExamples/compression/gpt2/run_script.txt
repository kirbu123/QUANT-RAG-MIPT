dataset_name: wikitext
dataset_config_name: wikitext-2-raw-v1
train_file: None
validation_file: None
validation_split_percentage: 5
model_name_or_path: openai-community/gpt2-large
path_to_model: None
config_name: None
tokenizer_name: None
use_slow_tokenizer: False
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
learning_rate: 5e-06
next_reg_lam: 0.1
weight_decay: 0.0
num_train_epochs: 5
max_train_steps: 1450
gradient_accumulation_steps: 1
lr_scheduler_type: linear
num_warmup_steps: 0
output_dir: /home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_new_layers
seed: 42
model_type: None
block_size: None
preprocessing_num_workers: None
overwrite_cache: False
no_keep_linebreaks: False
not_tie_wre: False
push_to_hub: False
hub_model_id: None
hub_token: None
data_folder: None
local_rank: 0
device: 0
smooth: True
use_prev_quant_layer_input: False
alpha: 0.5
smooth_dataset_path: /home/buka2004/PTQ-LLM-MIPT/smoothquant/datasets/val.jsonl.zst
smooth_output_path: /home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/out/ZeroQuant/W8A8_quantization_lkd_new_layers/act_scales/gpt2-large.pt
num_samples: 1024
seq_len: 128
deepspeed: True
deepspeed_config: /home/buka2004/PTQ-LLM-MIPT/DeepSpeedExamples/compression/gpt2/config/ds_config_W8A8_Qgroup64_fp32.json
deepscale: False
deepscale_config: None
